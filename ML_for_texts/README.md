_Список библиотек: lightgbm, matplotlib.pyplot, nltk,, numpy, pandas, re, tensorflow_hub, time, torch, transformers, google.colab, itertools, RandomForestClassifier, TfidfVectorizer, LogisticRegression, f1_score, train_test_split, GridSearchCV, make_pipeline, DecisionTreeClassifier, class_weight, TextBlob, Word, notebook, BertTokenizer, TFBertModel_

__Проект__

Обучение модели классификации комментариев на позитивные и негативные. 

В вашем распоряжении набор данных с разметкой о токсичности правок.

Значением метрики качества F1 должно быть не меньше 0.75. Данные находятся в файле /datasets/toxic_comments.csv(источник Яндекс.Практикум). Столбец text в нём содержит текст комментария, а toxic — целевой признак с разметкой о токсичности

__Что было сделано:__

Работа проводилась в Google Colab, т.к. мощностей ноутбука не хватало.

В первой части данные были подготовлены вручную:

- в связи с дисбалансом классов, рассчиталм параметр class_weight;
- избавились от стоп-слов;
- провели лемматизацию с указанием POS-тегов;
- текст привели к нижнему регистру;
- выборку разбили на обучающую и тестовую;
- с помощью пайплайна, кроссвалидации и векторизации подобрали лучшую модель из 3-х(logisticregression, decisiontreeclassifier, lgbmclassifier) с наилучшими гиперпараметрами;
- на самой успешной модели с подобранными гиперпараметрами LogisticRegression(class_weight=class_weights, solver='lbfgs', max_iter=1000, C=10, penalty='l2') провели обучение на векторизованных призаках и неизмененных целях.

Получили метрику 0.7562, что удобвлетворило условиям задачи.

Во второй части попробовали предобученную BERT-модель и Логистическую регрессию на трети от изначальной выборки без изменений.

Получили результат 0.7225. 